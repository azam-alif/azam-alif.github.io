<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MUNGE & SLURM Setup Guide</title>
  <link rel="icon" type="image/png" href="../favicon.png">
  <link rel="stylesheet" href="../style.css">
</head>
<body>
    <!-- Particles.js background -->
    <div id="particles-js"></div>

    <div class="container">
        <main class="main-content">
            <header>
                <h1>How To Set Up MUNGE and SLURM on Linux</h1>
                <nav>
                    <a href="../index.html">Home</a>&nbsp;
                    <a href="../topics.html">Blog Topics</a>&nbsp;
                </nav>
            </header>

            <main>
                <h2>What are MUNGE and SLURM?</h2>
                <p>
                    <b>MUNGE</b> (MUNGE Uid 'N' Gid Emporium) is an authentication service used in HPC clusters.
                    It allows nodes to securely verify each other using shared cryptographic credentials.
                </p>
                <p>
                    <b>SLURM</b> (Simple Linux Utility for Resource Management) is a powerful workload manager.
                    It handles job scheduling, resource allocation, and node management in clusters.
                </p>

                <p>
                    In a cluster environment, <b>MUNGE provides authentication</b> and
                    <b>SLURM uses that authentication</b> to safely communicate between the controller and compute nodes.
                </p>

                <hr>

                <h2>1. Master Node Configuration</h2>
                
                <h3>1.1. Munge</h3>

                <h4>Install MUNGE</h4>
                <ul><li>Arch Linux:</li></ul>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo pacman -S munge</code></pre>
                </div>

                <h4>Create the MUNGE Key</h4>
                <p>
                    The MUNGE key must be identical on <b>all nodes</b>. Create it only on the master node.
                </p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo create-munge-key</code></pre>
                </div>
                
                <h4>Set Permissions</h4>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo chown munge:munge /etc/munge/munge.key
sudo chmod 400 /etc/munge/munge.key</code></pre>
                </div>
                
                <p><b>Note on Time Synchronization:</b> For MUNGE to work, all nodes in the cluster must have their clocks synchronized. Authentication will fail if there is a significant time difference between nodes. It is recommended to use an NTP server to keep time consistent. For a guide on setting this up, see <a href="chrony.html">Setting Up a Chrony NTP Server</a>.</p>

                <h4>Enable and Start MUNGE on Master</h4>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo systemctl enable munge
sudo systemctl start munge</code></pre>
                </div>

                <h4>Test MUNGE on Master</h4>
                <p>Run this on the master node to confirm it is working locally.</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>munge -n | unmunge</code></pre>
                </div>
                
                <hr>
                
                <h3>1.2. Slurm</h3>

                <h4>Install SLURM</h4>
                <ul><li>Arch Linux:</li></ul>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo pacman -S slurm</code></pre>
                </div>

                <h4>Create SLURM User</h4>
                <p>SLURM runs under a dedicated system user.</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo useradd -r -s /usr/bin/nologin slurm</code></pre>
                </div>

                <h4>Configure slurm.conf</h4>
                <p>
                    Generate or edit the configuration file at <code>/etc/slurm-llnl/slurm.conf</code>. This file must be <b>identical on all nodes</b>.
                </p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>ClusterName=cluster
SlurmctldHost=archlinux

ProctrackType=proctrack/linuxproc
ReturnToService=1

SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817

SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd

SlurmUser=slurm

StateSaveLocation=/var/spool/slurmctld

TaskPlugin=task/affinity,task/cgroup

InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

SchedulerType=sched/backfill
SelectType=select/cons_tres

JobCompType=jobcomp/none
JobAcctGatherFrequency=30

SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log

NodeName=node1 NodeAddr=192.168.144.199 CPUs=4 RealMemory=3300 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1
NodeName=node2 NodeAddr=192.168.144.102 CPUs=4 RealMemory=3300 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1

PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP
</code></pre>
                </div>

                <h4>Create Required Directories on Master</h4>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo mkdir -p /var/spool/slurmctld
sudo chown slurm:slurm /var/spool/slurmctld</code></pre>
                </div>
                
                <h4>Enable and Start SLURM Controller</h4>
                <p>On the controller (master) node:</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo systemctl enable slurmctld
sudo systemctl start slurmctld</code></pre>
                </div>

                <hr>

                <h2>2. Compute Node Configuration</h2>
                <p>
                    The following steps assume you are provisioning your compute nodes using a network file system (NFS) where the root directory for nodes is, for example, under <code>/srv/nfsroot/</code>. Adjust the paths accordingly. These commands would be run on the master node, targeting the compute node's filesystem.
                </p>

                <h3>2.1. Munge Setup and Check</h3>
                
                <h4>Copy MUNGE Key to Compute Node</h4>
                <p>Copy the MUNGE key from the controller to the compute node's filesystem.</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo cp /etc/munge/munge.key /srv/nfsroot/etc/munge/munge.key</code></pre>
                </div>
                <p>You must also ensure the permissions are correct on the compute node, and the munge service is started.</p>
                
                <h4>Test MUNGE Communication</h4>
                <p>From the master node, test authentication with a compute node (e.g., `node1`):</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>munge -n | ssh node1 unmunge</code></pre>
                </div>
                <p>This should return a status line showing success.</p>
                
                <h3>2.2. Slurm Setup and Check</h3>

                <h4>Copy SLURM Configuration</h4>
                <p>The <code>slurm.conf</code> file must also be present on all compute nodes.</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo cp /etc/slurm-llnl/slurm.conf /srv/nfsroot/etc/slurm-llnl/slurm.conf</code></pre>
                </div>

                <h4>Create SLURM Directories on Compute Node</h4>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo mkdir -p /srv/nfsroot/var/spool/slurmd
sudo chown slurm:slurm /srv/nfsroot/var/spool/slurmd</code></pre>
                </div>

                <h4>Enable SLURM Daemon on Compute Nodes</h4>
                <p>On each compute node, enable and start the <code>slurmd</code> service:</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sudo systemctl enable slurmd
sudo systemctl start slurmd</code></pre>
                </div>
                
                <h4>Check Cluster Status</h4>
                <p>From the controller node, you can now check the status of the cluster and its nodes.</p>
                <div class="code-block-container">
                    <button class="copy-code-button" onclick="copyCode(this)">Copy</button>
<pre><code>sinfo
scontrol show nodes</code></pre>
                </div>

                <p>
                    If nodes appear as <code>idle</code>, your SLURM cluster is working correctly.
                </p>

                <hr>
                <footer>
                  <p>Posted on: <b>December 13, 2025</b></p>
                </footer>
            </main>
        </main>
    </div>

    <!-- Scripts for particles.js -->
    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script src="../script.js"></script>
</body>
</html>

